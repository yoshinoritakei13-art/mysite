{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### RAGアプリ\n","\n",">アプリの概要\n","\n","<img src=\"pic/rag.png\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">目的\n","\n","- RAGの仕組みの理解\n","    - ベクトル検索（Embedding）\n","- 基本的なRAGの実装\n","- 特定のWebページの情報を元に必要な回答を引き出すRAGの実装"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">RAGのイメージ\n","\n","<img src=\"pic/rag1.png\">\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">RAGにおけるデータソース\n","\n","<img src=\"pic/rag3.png\">\n","\n","<img src=\"pic/rag6.png\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">キーワード検索とベクトル検索\n","\n","<img src=\"pic/rag5.png\">\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">ベクトル検索\n","\n","<img src=\"pic/rag2.png\">\n","<img src=\"pic/rag4.png\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">RAGとEmbedding\n","\n","・ Embedding：テキストを高次元ベクトル化すること。\n","\n","・ RAG：データソースから取得した情報をベースにLLMが回答を生成する手法。\n"," - 両者は全く別ものであるが、組み合わせることで、「ベクトルデータベースを利用したRAG」を構築することが可能。\n"," - ベクトルデータベースを利用すれば、Embeddingによって高次元ベクトル化されたデータを格納することで、類似度検索エンジンを構築することができる。\n"," - RAGにおいて、ベクトルデータベースを利用するやり方が現在ポピュラーになっている。\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">RAGのEmbeddingの作業の流れ\n","\n","1. テキストのEmbedding\n"," - RAGでは、まず、データベース内の文書やユーザーからのクエリを数値のベクトルに変換します。\n"," - この変換プロセスによって、テキストデータが機械学習モデルが解釈しやすい形式になります。\n","\n","2. 関連情報の検索\n"," - Embeddingされたクエリベクトルを使用して、データベース内から関連する文書や情報を検索します。\n"," - このとき、クエリベクトルと文書ベクトル間の類似性を計算し、最も関連性の高い文書を特定します。\n","\n","3. テキスト生成のための情報の統合\n"," - 検索された文書や情報は、読みやすい形式に変換された後、元のユーザーのクエリやプロンプトと組み合わせされます。\n"," - この強化されたプロンプトが、LLMに入力され、テキスト生成の基盤となります。\n","\n","4. テキストの生成\n"," - LLMは、強化されたプロンプトを基にして、関連性の高い回答やテキストを生成します。\n"," - このプロセスにより、ユーザーの質問に対する詳細で関連性の高い回答を提供することが可能になります。"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[">動作を構成してる部品の紹介（APIやライブラリ）\n","\n","API\n","- openAIのchatGPT\n","    - 目的：AIのモデルを使う。\n","    - 参考：https://platform.openai.com/docs/api-reference/introduction?lang=python\n","\n","- openAIのEmbedding\n","    - 目的：言語情報をベクトル化し、効果的に機械学習に利用する\n","    - 参考：https://weel.co.jp/media/tech/text-embedding-3/\n","\n","ライブラリ\n","- streamlit\n","    - 目的：特定のWebページの情報を元に必要な回答を引き出すためのUIを表示するためのフロントエンドを生成する\n","- openai\n","    - 目的：openAIのchatGPT、Embeddingを活用するためのライブラリ\n","- requests\n","    - 目的：HTTPリクエストを送信するためのライブラリ。WebページやAPIからデータを取得する際に使用される。\n","- BeautifulSoup\n","    - 目的：HTMLやXMLを解析し、必要な情報を抽出するためのライブラリ。requestsで取得したHTMLデータを構造化し、検索・抽出しやすくするために使用される。\n","- cosine_similarity\n","    - 目的：ベクトル間のコサイン類似度を計算するためのライブラリ。2つのベクトルがどれだけ類似しているかを評価するために使用される。\n","\n",">ライブラリのインストール\n","\n","ライブラリ\n","- streamlit\n","    - pip install streamlit\n","- openai\n","    - pip install openai\n","- requests\n","    - pip install requests\n","- BeautifulSoup\n","    - pip install beautifulsoup4\n","- cosine_similarity\n","    - pip install scikit-learn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2772,"status":"ok","timestamp":1714807745698,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"h_H-R4Q3bx6o"},"outputs":[],"source":["import streamlit as st # フロントエンドを扱うstreamlitの機能をインポート\n","from openai import OpenAI  # OpenAIライブラリをインポートして、OpenAIのAPIとやり取りする\n","import os                  # 環境変数を管理するためにosをインポート\n","import requests  # Webリクエストを行うためのライブラリ\n","from bs4 import BeautifulSoup  # HTMLを解析するためのBeautifulSoupライブラリをインポート\n","from sklearn.metrics.pairwise import cosine_similarity  # cosine_similarityをインポートして類似度を計算\n","\n","# OpenAI APIキーを環境変数として設定\n","os.environ[\"OPENAI_API_KEY\"] = 'ご自身のOPENAI_API_KEYを入力'\n","\n","# OpenAIクライアントを初期化\n","client = OpenAI()\n","\n","def vectorize_text(text):\n","    \"\"\"\n","    テキスト文字列を、OpenAIの埋め込みモデルを使用して対応するベクトルに変換する関数。\n","    \"\"\"\n","    response = client.embeddings.create(\n","        input=text,                      # 埋め込みに変換するテキスト\n","        model=\"text-embedding-3-small\"   # 使用する埋め込みモデルを指定\n","    )\n","    return response.data[0].embedding    # レスポンスから埋め込みベクトルを抽出して返す\n","\n","# 質問を定義、この質問に最も類似した文書を見つけたい\n","question = \"2023年の第1事業部の売上はどのくらい？\"\n","\n","# 質問と比較するための文書のリストを定義\n","documents = [\n","    \"2023年上期売上200億円、下期売上300億円\",\n","    \"2023年第1事業部売上300億円、第2事業部売上150億円、第3事業部売上50億円\",\n","    \"2024年は全社で1000億円の売上を目指す\"\n","]\n","\n","# リスト内の各文書に対して埋め込みベクトルを生成\n","vectors = [vectorize_text(doc) for doc in documents]\n","\n","# 質問に対して埋め込みベクトルを生成\n","question_vector = vectorize_text(question)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714808415668,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"c69RVRUcc_k0","outputId":"0cb4630a-7850-493a-9a47-785d9f03a0fe"},"outputs":[],"source":["\n","max_similarity = 0  # 最大の類似度を格納するための変数を初期化\n","most_similar_index = 0  # 最も類似している文書のインデックスを格納するための変数を初期化\n","\n","# すべての文書ベクトルを順番にチェック\n","for index, vector in enumerate(vectors):\n","    # 質問ベクトルと現在の文書ベクトルとのコサイン類似度を計算\n","    similarity = cosine_similarity([question_vector], [vector])[0][0]\n","    \n","    # 現在の文書とその類似度を表示\n","    print(documents[index], \":\", similarity)\n","    \n","    # 計算された類似度がこれまでの最大類似度より大きい場合\n","    if similarity > max_similarity:\n","        max_similarity = similarity  # 最大類似度を更新\n","        most_similar_index = index   # 最も類似している文書のインデックスを更新\n","\n","# 最も類似している文書を表示\n","print(documents[most_similar_index])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1374,"status":"ok","timestamp":1714809309489,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"Und1CW-alPsQ","outputId":"797b1487-a446-4bfa-8acc-5f134d09ec22"},"outputs":[],"source":["# ユーザーの質問と最も類似している文書を使ってプロンプトを作成\n","prompt = f'''以下の質問に以下の情報をベースにして答えて下さい。\n","[ユーザーの質問]\n","{question}\n","\n","[提供された情報]\n","{documents[most_similar_index]}\n","'''\n","\n","# GPT-4o-miniモデルを使用して、ユーザーの質問に基づいて回答を生成\n","response = client.chat.completions.create(\n","    model=\"gpt-4o-mini\",  # 使用するモデルを指定\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # システムメッセージでアシスタントの役割を設定\n","        {\"role\": \"user\", \"content\": prompt}  # ユーザーの質問と提供された情報を含むプロンプトを渡す\n","    ],\n","    max_tokens=200  # レスポンスの最大トークン数を設定\n",")\n","\n","# モデルが生成した回答を表示\n","print(response.choices[0].message.content)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lDYjsNCcElUA"},"source":["## 特定のURLの情報にRAGを使ってアクセスしてチャットボットから回答を得る"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2154,"status":"ok","timestamp":1714868181853,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"R8nnotn7ErD4"},"outputs":[],"source":["import requests  # Webリクエストを行うためのライブラリをインポート\n","from bs4 import BeautifulSoup  # HTMLを解析するためのBeautifulSoupライブラリをインポート\n","\n","# 解析するためのWebページのURLを設定\n","url = \"https://tech0-jp.com/#program\"\n","\n","# 指定されたURLに対してHTTP GETリクエストを送信し、レスポンスを取得\n","response = requests.get(url)\n","\n","# レスポンスのHTMLコンテンツをBeautifulSoupを使って解析\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# ページ内のすべての<div>要素を取得\n","text_nodes = soup.find_all(\"div\")\n","\n","# すべての<div>要素のテキストを連結し、タブや改行を削除\n","joined_text = \"\".join(t.text.replace(\"\\t\", \"\").replace(\"\\n\", \"\").replace(\" \", \"\") for t in text_nodes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"elapsed":382,"status":"ok","timestamp":1714868183592,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"uUCNWMPzFfeY","outputId":"336122bc-d8a7-4c95-a29c-0ed79654fa4c"},"outputs":[],"source":["joined_text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714868629080,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"R80599WXGrnu","outputId":"94ee4d7b-8ab7-4ef9-bff0-c6f8a4414520"},"outputs":[],"source":["len(joined_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1714869550868,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"eC8UDcJNJ8Ec"},"outputs":[],"source":["chunk_size = 400  # 各チャンクのサイズを400文字に設定\n","overlap = 50  # 各チャンク間で重複する文字数を50文字に設定\n","chunks = []  # テキストチャンクを格納するためのリストを初期化\n","start = 0  # チャンクの開始位置を初期化\n","\n","# テキストの長さがチャンクサイズを超えるまで繰り返し処理を行う\n","while start + chunk_size <= len(joined_text):\n","    # 現在の開始位置からチャンクサイズ分のテキストを切り出してリストに追加\n","    chunks.append(joined_text[start:start + chunk_size])\n","    # 次のチャンクの開始位置を更新（重複部分を除く）\n","    start += (chunk_size - overlap)\n","\n","# 残りのテキストがある場合、それを最後のチャンクとしてリストに追加\n","if start < len(joined_text):\n","    chunks.append(joined_text[-chunk_size:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325,"status":"ok","timestamp":1714869624333,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"MaHM1oaHMLop","outputId":"96eb3189-31b7-41db-9d6d-071d214c8188"},"outputs":[],"source":["# 各チャンクを順番に処理するためにループを設定\n","for i, chunk in enumerate(chunks):\n","    # チャンクの番号と、そのチャンクの最初の50文字と最後の50文字を表示\n","    # チャンク番号は1から始めるようにiに1を足している\n","    print(f\"Chunk {i+1} : {chunk[:50]}...{chunk[-50:]}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ISW4IvZsQBvB"},"source":["## 特定のURLの情報を元に作ったチャンクをベースにRAG"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2190,"status":"ok","timestamp":1714872855897,"user":{"displayName":"Yuma Ueno","userId":"18156603877946075290"},"user_tz":-540},"id":"EQv7rdkOMdml","outputId":"d76e45c5-932a-4f92-c9ad-46776b7a4a7f"},"outputs":[],"source":["import streamlit as st # フロントエンドを扱うstreamlitの機能をインポート\n","import requests  # Webリクエストを行うためのライブラリ\n","from bs4 import BeautifulSoup  # HTMLを解析するためのライブラリ\n","from openai import OpenAI  # OpenAIのAPIを利用するためのライブラリ\n","import os  # 環境変数の管理のためのライブラリ\n","from sklearn.metrics.pairwise import cosine_similarity  # コサイン類似度を計算するための関数\n","\n","# OpenAI APIキーを環境変数に設定\n","os.environ[\"OPENAI_API_KEY\"] = 'ご自身のOPENAI_API_KEYを入力'\n","client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))  # OpenAIクライアントを初期化\n","\n","# 指定されたURLから記事をスクレイピングし、テキストを返す関数\n","def scrape_article(url):\n","    response = requests.get(url)  # 指定されたURLにGETリクエストを送信\n","    soup = BeautifulSoup(response.text, \"html.parser\")  # 取得したHTMLを解析\n","    text_nodes = soup.find_all(\"div\")  # すべての<div>タグを取得\n","    joined_text = \"\".join(t.text.replace(\"\\t\", \"\").replace(\"\\n\", \"\") for t in text_nodes)  # テキストを連結し、タブや改行を削除\n","    return joined_text  # 連結されたテキストを返す\n","\n","# テキストをチャンクに分割する関数\n","def chunk_text(text, chunk_size, overlap):\n","    chunks = []  # チャンクを格納するリストを初期化\n","    start = 0  # チャンクの開始位置を初期化\n","    while start + chunk_size <= len(text):\n","        chunks.append(text[start:start + chunk_size])  # チャンクをリストに追加\n","        start += (chunk_size - overlap)  # 次のチャンクの開始位置を設定\n","    if start < len(text):\n","        chunks.append(text[-chunk_size:])  # 残りのテキストがある場合は最後のチャンクとして追加\n","    return chunks  # 分割されたチャンクのリストを返す\n","\n","# テキストをベクトル化する関数\n","def vectorize_text(text):\n","    response = client.embeddings.create(\n","        input=text,\n","        model=\"text-embedding-ada-002\"  # 正しいモデル名を指定\n","    )\n","    return response.data[0].embedding  # 生成された埋め込みベクトルを返す\n","\n","# 質問ベクトルと文書ベクトルを比較して最も類似した文書を見つける関数\n","def find_most_similar(question_vector, vectors, documents):\n","    similarities = []  # 類似度を格納するリストを初期化\n","    for index, vector in enumerate(vectors):\n","        similarity = cosine_similarity([question_vector], [vector])[0][0]  # コサイン類似度を計算\n","        similarities.append([similarity, index])  # 類似度と文書のインデックスをリストに追加\n","    similarities.sort(reverse=True, key=lambda x: x[0])  # 類似度の降順でソート\n","    top_documents = [documents[index] for similarity, index in similarities[:2]]  # 上位2つの文書を取得\n","    return top_documents  # 最も類似した文書を返す\n","\n","# 質問を基にGPT-4モデルで回答を生成する関数\n","def ask_question(question, context):\n","    # 質問と文脈情報を基にプロンプトを作成\n","    prompt = f'''以下の質問に以下の情報をベースにして答えて下さい。\n","    [ユーザーの質問]\n","    {question}\n","\n","    [情報]\n","    {context}\n","    '''\n","    print(prompt)  # 作成したプロンプトを表示（デバッグ用）\n","\n","    # OpenAIのチャットモデルにリクエストを送信して回答を生成\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",  # 正しいモデル名を指定\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # システムメッセージでアシスタントの役割を設定\n","            {\"role\": \"user\", \"content\": prompt}  # ユーザーからの質問と文脈情報を含むプロンプトを渡す\n","        ],\n","        max_tokens=400  # レスポンスの最大トークン数を設定\n","    )\n","    return response.choices[0].message.content  # 生成された回答のテキストを返す\n","\n","# 記事のURLとテキストチャンク分割のパラメータを設定\n","url = \"https://tech0-jp.com/#program\"\n","chunk_size = 400\n","overlap = 50\n","\n","# 指定されたURLから文章をスクレイピングし、テキストチャンクに分割\n","article_text = scrape_article(url)\n","text_chunks = chunk_text(article_text, chunk_size, overlap)\n","\n","# 各テキストチャンクをベクトル化\n","vectors = [vectorize_text(doc) for doc in text_chunks]\n","\n","# 質問をベクトル化し、最も類似した文書を検索\n","question = \"Step2ではどのようなことが身につけられますか？\"\n","question_vector = vectorize_text(question)\n","similar_document = find_most_similar(question_vector, vectors, text_chunks)\n","\n","# 質問を基に回答を生成\n","answer = ask_question(question, similar_document)\n","print(answer)  # 生成された回答を表示"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOTDN9Hlm5c+OUG9c5BHvZB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
