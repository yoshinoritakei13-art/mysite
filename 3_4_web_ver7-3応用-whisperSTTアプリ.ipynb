{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3webアプリ作成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目次：https://docs.google.com/document/d/1eDw9PK5Ft0vImu81e7oERB_q-dZvxbFe8bGetZBgEEg/edit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 応用実装"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAIのSTTモデルであるwhisperを使ったアプリ\n",
    "\n",
    ">アプリの概要\n",
    "\n",
    "<img src=\"pic/img052.png\">\n",
    "\n",
    "\n",
    "\n",
    ">動作を構成してる部品の紹介（APIやライブラリ）\n",
    "API\n",
    "- openAIのchatGPT\n",
    "    - 目的：AIのモデルを使う。\n",
    "    - 参考：https://platform.openai.com/docs/api-reference/introduction?lang=python\n",
    "\n",
    "ライブラリ\n",
    "- streamlit\n",
    "    - 目的：chatGPTにリクエストをするためのUIを表示するためのフロントエンドを生成する\n",
    "- openai\n",
    "    - 目的：openAIのchatGPTを活用するためのライブラリ\n",
    "- audio_recorder_streamlit\n",
    "    - 目的：Streamlitアプリケーション内でオーディオの録音を可能にするためのライブラリ\n",
    "- wave\n",
    "    - 目的：WAV形式のオーディオファイルを操作するためのPython標準ライブラリ\n",
    "\n",
    "\n",
    ">ライブラリのインストール\n",
    "\n",
    "ライブラリ\n",
    "- streamlit\n",
    "    - pip install streamlit\n",
    "- openai\n",
    "    - pip install openai\n",
    "- audio_recorder_streamlit\n",
    "    - pip install audio_recorder_streamlit\n",
    "\n",
    "\n",
    ">ライブラリの公式ドキュメントの見方や使い方を説明\n",
    "\n",
    "- API\n",
    "    - openAIのchatGPTを活用するためのapi_keyの取得\n",
    "        - openAIのapi_keyが必要なので、登録してapi_keyを取得しましょう。\n",
    "        - 参考：https://openai.com/\n",
    "\n",
    "\n",
    "ライブラリ\n",
    "\n",
    "- streamlit\n",
    "    - インストール方法はget startページで確認\n",
    "    - 参考：https://docs.streamlit.io/library/get-started/installation\n",
    "    - st.titleなどはコンポーネントページから確認出来る\n",
    "    - 参考：https://streamlit.io/components\n",
    "- openai\n",
    "    - 今回はSTTのモデルであるwhisper使って文字お越しを行います。\n",
    "    - client.audio.transcriptions.createを使ってAIにリクエストします。\n",
    "    - 参考：https://platform.openai.com/docs/guides/speech-to-text/quickstart\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">それぞれの機能をjupyterで動く最小限を作る"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下がjuptyterにて動く最低限のアプリの流れです。\n",
    "\n",
    "1. 作るアプリの全体のコードを把握\n",
    "1. それぞれ機能を解説\n",
    "    1. chatGPTの仕様\n",
    "    1. chatGPTの使い方\n",
    "    1. バージョンやAPIキーの設定方法の補足"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.全体のコード"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使うアプリの全体像は下記のようになります。  \n",
    "これはweb上では動きませんが、手元では動作するアプリです。\n",
    "\n",
    "※このアプリの動作にはAPIキーが必要です。  \n",
    "openAIに登録して、取得したAPIキーの文字列を必ずos.environ[\"OPENAI_API_KEY\"]に代入してください。\n",
    "\n",
    "以下のコードの動作の流れ\n",
    "1. 拡張機能であるライブラリをインポート\n",
    "1. openAIのGPTにアクセスするためのAPIキーを設定\n",
    "1. 音声を録音するための関数を用意\n",
    "    1. audio_recorder 関数を使って音声を録音し、その結果を返します。  \n",
    "    ＊引数として、録音のパラメーター（energy_threshold、pause_threshold、sample_rate）と録音の開始時に表示されるテキスト (text) を指定しています。\n",
    "1. 録音結果を取得して、処理を行う\n",
    "    1. 録音が成功したかどうかを判定します  \n",
    "    　ー録音が成功した場合（contents が None でない場合）  \n",
    "    　    st.audio(contents) を使って録音された音声を表示し、同時にそのデータを WAV ファイルに保存   \n",
    "    　ー録音が失敗した場合  \n",
    "    　    ユーザーに対して録音手順の案内を提供し、プログラムを停止します。  \n",
    "1. GPTにリクエスト  \n",
    "    1. それぞれの変数に、モデル（\"whisper-1\"）,録音結果（audio_file）を代入します\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">（演習）streamlitに実装しましょう！(今回は応用編なのでいきなり実装です！)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この演習では必ず実行ファイルにコーディングしてください。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※streamlit run ファイル名で実行する時は必ず実行ファイル（.py）で実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_recorder_streamlit import audio_recorder\n",
    "# import speech_recognition as sr\n",
    "import wave\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ローカルの場合\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# streamlit cloudに登録する場合はこちらを使用\n",
    "#os.environ[\"OPENAI_API_KEY\"] = st.secrets.api.key\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def recorder():\n",
    "    contents = audio_recorder(\n",
    "        energy_threshold = (1000000000,0.0000000002), \n",
    "        pause_threshold=0.1, \n",
    "        sample_rate = 48_000,\n",
    "        text=\"Clickして録音開始　→　\"\n",
    "    )\n",
    "    return contents\n",
    "\n",
    "contents = recorder()\n",
    "if contents == None:\n",
    "    st.info('①　アイコンボタンを押して回答録音　(アイコンが赤色で録音中)。  \\n②　もう一度押して回答終了　(再度アイコンが黒色になれば完了)')\n",
    "    st.error('録音完了後は10秒程度お待ちください。')\n",
    "    st.stop()\n",
    "else:\n",
    "\n",
    "\n",
    "    wave_data = st.audio(contents)\n",
    "    print(type(contents)) #　bytesデータ\n",
    "\n",
    "    with wave.open(\"audio.wav\", \"wb\") as audio_file:\n",
    "        audio_file.setnchannels(2)\n",
    "        audio_file.setsampwidth(2)\n",
    "        audio_file.setframerate(48000)\n",
    "        audio_file.writeframes(contents)\n",
    "\n",
    "    # r = sr.Recognizer()\n",
    "    # with sr.AudioFile(\"audio.wav\") as source:\n",
    "    #     audio_data = r.record(source)\n",
    "    #     recognized_text = r.recognize_google(audio_data, language=\"ja-JP\")\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    audio_file= open(\"./audio.wav\", \"rb\")\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    )\n",
    "    recognized_text = transcription.text\n",
    "        \n",
    "    st.write(recognized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9615d5aa5a330c24de68476d8cc9e13337aa201710c2e0802369a06b82c4cd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
